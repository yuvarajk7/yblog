{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Blog</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:jupyter-notebook","title":"Jupyter Notebook","text":"<ul> <li>            Using Jupyter from VS code or Standalone project          </li> </ul>"},{"location":"tags/#tag:llm","title":"LLM","text":"<ul> <li>            LLM Models and Parameters          </li> </ul>"},{"location":"tags/#tag:mcp","title":"MCP","text":"<ul> <li>            Configure MCP Server in Visual Studio and VS Code          </li> </ul>"},{"location":"tags/#tag:uv","title":"uv","text":"<ul> <li>            Create a Python Project Using uv and Manage Dependencies          </li> <li>            Python Package and Project Manager Tool \u2013 uv          </li> </ul>"},{"location":"2025/09/06/2025-09-06-introduction/","title":"2025 09 06 Introduction","text":""},{"location":"2025/09/06/2025-09-06-introduction/#about-me","title":"\ud83d\udc66About Me.","text":"<p>I\u2019m Yuvaraj Kesavan, a hands-on engineering leader with over two decades of experience building scalable, distributed applications across healthcare, RCM, and enterprise domains. I specialize in architecting cloud-native solutions on Azure, modernizing monolithic systems into microservices, and designing secure, interoperable APIs. With a strong background in ASC and hospital domains, I\u2019ve led teams to deliver high-performance applications, achieving millions of transactions per day. I\u2019m also passionate about mentoring teams, driving full product life cycle delivery, and fostering cross-functional collaboration to transform complex challenges into successful outcomes.  \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d\ud83c\udf89</p>"},{"location":"2025/09/09/llm-models-and-parameters/","title":"LLM Models and Parameters","text":"","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#models","title":"Models","text":"<p>Many LLM models are available on the market, which can make it overwhelming to choose the right one. In this article, we will look at the list of LLM categories, their functionalities, and important parameters for fine-tuning LLMs. LLMs revolve around the Transformer architecture, which powers many generative AI applications like chats, summarization, and translation. This architecture has three important components:</p> <ol> <li>Tokenizer: Converts text into tokens.</li> <li>Embedding: Turns tokens into vectors representing meaning and position.</li> <li>Transformer: Uses encoders, decoders, and self-attention to process and generate text.</li> </ol> <p></p>","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#transformer-architectures","title":"Transformer Architectures","text":"<p>The Transformer supports three main architectures:</p> <ol> <li>Encoder-only: Best for understanding and classifying text (e.g., <code>BERT</code>, <code>RoBERTa</code>, <code>DistilBERT</code>).</li> <li>Decoder-only: Best for generating new text from prompts (e.g., <code>GPT</code>, <code>Llama</code>, <code>Mistral</code>).</li> <li>Encoder-decoder: Versatile for tasks like translation, summarization, and Q&amp;A (e.g., <code>T5</code>, <code>BART</code>).</li> </ol> <p>OpenAI APIs make it easy to use powerful language models without managing infrastructure. In contrast, the Hugging Face Transformers library allows for more customization and direct model control, including fine-tuning. Both are used for integrating AI into applications.</p>","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#model-parameters","title":"Model Parameters","text":"<p>Below are the key parameters that influence text generation with Transformers:</p> <ul> <li><code>Prompt</code>: Sets the context for the model\u2019s output. In chat models, prompts are split into <code>system</code> (instructions/role), <code>user</code> (questions/commands), and <code>assistant</code> (model\u2019s responses).</li> <li><code>max_new_tokens</code>: Limits the number of tokens generated in the output, controlling the output length.</li> <li><code>max_length</code>: Limits the total length of the input plus the output. Use either <code>max_new_tokens</code> or <code>max_length</code>, not both.</li> <li><code>temperature</code>: Controls randomness. Higher values (closer to 1) make outputs more creative and varied; lower values make them more predictable and focused.</li> <li><code>do_sample</code>: If <code>false</code>, the model always picks the most likely next token (deterministic). If <code>true</code>, it samples from the probability distribution, resulting in more creative output.</li> <li><code>top_k</code>: When sampling, this restricts choices to the K most likely tokens. Lower values lead to more focused output, while higher values create more diverse results.</li> <li><code>top_p</code>: Samples from the smallest set of tokens whose cumulative probability exceeds a threshold, which allows for more dynamic and varied outputs.</li> </ul> <p>Other parameters (like beam search, repetition penalties, etc.) offer further control. More details can be found in the Hugging Face Transformers documentation. These parameters let you balance creativity, coherence, and length in generated text, depending on your application\u2019s needs.</p>","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#categories-of-models","title":"Categories of Models","text":"","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#code-generation-models","title":"Code Generation Models","text":"<p>Code generation tools like GitHub Copilot and Cursor are powered by Transformer models trained on vast code datasets from sources like GitHub, Stack Overflow, and documentation. Many LLMs use reinforcement learning with human feedback (RLHF) to align outputs with professional coding standards. There are two main categories of code generation models:</p> <ul> <li>General-purpose LLMs (e.g., <code>GPT-4</code>, <code>Claude</code>): These can generate both natural language and code, assisting with code completion, error correction, and test creation. They are best for prototyping and tasks involving both code and natural language.</li> <li>Specialized code models (e.g., <code>CodeLlama</code>, <code>CodeT5</code>, <code>CodeBERT</code>): Trained specifically on code, these models excel at advanced tasks such as code search, clone detection, bug detection, and code translation. They are preferred for high-precision or domain-specific code tasks.</li> </ul>","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#image-generation-models","title":"Image Generation Models","text":"<p>DALL\u00b7E and Stable Diffusion create images from natural language prompts using diffusion, a process where noise is gradually removed to form a coherent image.</p> <ul> <li>DALL\u00b7E (by OpenAI):<ul> <li>Key parameters:<ul> <li><code>quality</code>: <code>HD</code> or <code>standard</code></li> <li><code>size</code>: Output image dimensions</li> <li><code>style</code>: <code>vivid</code> (hyperrealistic) or <code>natural</code> (authentic look)</li> </ul> </li> </ul> </li> <li>Stable Diffusion (open source):<ul> <li>Key parameters:<ul> <li><code>seed</code>: Ensures reproducibility of images.</li> <li><code>negative prompt</code>: Excludes unwanted elements.</li> <li><code>width</code>/<code>height</code>: Image dimensions (optimized for 512x512).</li> <li><code>steps</code>: Number of denoising steps (more steps = more detail).</li> <li><code>scheduler</code>: Controls the denoising rhythm, affecting style and quality.</li> <li><code>CFG scale</code>: Balances creativity and prompt adherence.</li> </ul> </li> </ul> </li> </ul>","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#text-to-speech-tts-models","title":"Text-to-Speech (TTS) Models","text":"<p>TTS converts written text into natural-sounding audio. The TTS pipeline involves several steps:</p> <ol> <li>Text is cleaned and normalized.</li> <li>Words are mapped to phonemes, and prosody (rhythm, stress, intonation) is analyzed.</li> <li>Acoustic modeling generates a mel spectrogram.</li> <li>A neural vocoder (like <code>WaveNet</code> or <code>HiFi-GAN</code>) converts the spectrogram to raw audio.</li> <li>Optional post-processing can be applied.</li> </ol> <p>Two popular TTS options are:</p> <ul> <li>OpenAI\u2019s TTS API: Simple and high-quality.<ul> <li>Key Parameters:<ul> <li><code>voice</code>: Choose from preset voices (e.g., <code>Nova</code>, <code>Alloy</code>).</li> <li><code>speed</code>: Controls speech rate.</li> <li><code>instructions</code>: Guide delivery style (e.g., \"excited,\" \"positive\").</li> </ul> </li> </ul> </li> <li>Chatterbox (Resemble AI): Open source and offers more control.<ul> <li>Key Parameters:<ul> <li><code>exaggeration</code>: Controls expressiveness.</li> <li><code>CFG weight</code>: Balances faithfulness to text.</li> <li><code>audio prompt path</code>: Reference audio for voice adaptation.</li> </ul> </li> </ul> </li> </ul>","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#multimodal-models-mllms","title":"Multimodal Models (MLLMs)","text":"<p>Multimodal models (MLLMs) can process and understand multiple data types (text, images, audio) within a single system. Training is a two-phase process: aligning new modalities and then fine-tuning the full system.</p> <ul> <li>Unified Embedding Approach: Modalities are encoded into the same embedding space as text. This is simple but can lose detail. Used in models like <code>Pix2Struct</code> and <code>Fuyu</code>.</li> <li>Cross-Attention Approach: Image encodings are connected to the LLM via cross-attention layers, allowing selective focus. This is more complex but more powerful. Used in models like <code>Flamingo</code> and <code>LLaVA</code>.</li> </ul>","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#fine-tuning-models","title":"Fine-tuning Models","text":"<p>Fine-tuning large models is resource-intensive. Parameter-Efficient Fine-Tuning (PEFT) techniques adapt models by updating only a small number of parameters. The benefits include:</p> <ul> <li>Much lower memory and compute requirements.</li> <li>Tiny storage for task-specific parameters.</li> <li>Preservation of original model knowledge.</li> </ul>","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#popular-peft-methods","title":"Popular PEFT methods:","text":"<ul> <li><code>LoRA</code> (Low-Rank Adaptation): Adds small, trainable matrices to each layer.</li> <li><code>QLoRA</code> (Quantized Low-rank Adaptation): Combines <code>LoRA</code> with quantization to fine-tune very large models on a single GPU.</li> <li>Prompt Tuning: Learns a small set of trainable embeddings (\u201csoft prompts\u201d) prepended to the input.</li> <li>Prefix Tuning: Injects trainable vectors (\u201cprefixes\u201d) into the attention mechanism of each layer.</li> </ul>","tags":["LLM"]},{"location":"2025/09/09/llm-models-and-parameters/#costs","title":"Costs","text":"<p>Model choice is only part of the challenge; cost is equally important.</p> <ul> <li>Hosted APIs (e.g., OpenAI, Anthropic, Google):<ul> <li>Easy setup with no infrastructure to manage.</li> <li>Pricing is usually per token (text) or per request (image/audio).</li> <li>Use pricing calculators and token-counting libraries (like <code>tiktoken</code>) to estimate costs.</li> </ul> </li> <li>Self-hosting open-source models (e.g., Llama, Mistral):<ul> <li>More control and flexibility, but you manage the infrastructure.</li> <li>Costs are based on GPU rental (hourly billing).</li> <li>Use tools like Hugging Face\u2019s model memory estimator to plan VRAM</li> </ul> </li> </ul>","tags":["LLM"]},{"location":"2025/09/10/configure-mcp-server-in-visual-studio-and-vs-code/","title":"Configure MCP Server in Visual Studio and VS Code","text":"<p>This article explains how to configure MCP servers in Visual Studio and Visual Studio Code. The Model Context Protocol (MCP) enables servers to expose tools and services that MCP clients \u2014 such as GitHub Copilot or Claude \u2014 can consume.</p> <p>This guide demonstrates how to configure the MCP Database server, which queries a SQL Server database and returns results to a chat agent. The agent can then interact with a large language model (LLM) to display results in the chat window or perform actions such as generating SQL or saving query results to a CSV file.</p> <p>The MCP database server used in this guide is a Node package and can be installed globally with npm:</p> <pre><code>npm install -g @executeautomation/database-server\n</code></pre>","tags":["MCP"]},{"location":"2025/09/10/configure-mcp-server-in-visual-studio-and-vs-code/#vs-code-configuration","title":"VS Code configuration","text":"<ol> <li>Open VS Code and press Ctrl+Shift+P. Run <code>MCP: Add Server\u2026</code>.</li> <li>Select the MCP server type. Choose \"Command (stdio)\" from the list (other types include http, npm, pip, and docker).</li> <li>For the command, enter: <code>npx</code>.</li> <li>For Server Id, enter: <code>advworks-db</code>.</li> <li>Choose the installation scope: Global or Workspace.</li> <li>Global \u2014 available to all workspaces and runs locally<ul> <li>This creates <code>mcp.json</code> at <code>C:\\Users\\&lt;user&gt;\\AppData\\Roaming\\Code\\User\\mcp.json</code>.</li> </ul> </li> <li>Workspace \u2014 available only in the current workspace and runs locally<ul> <li>This creates <code>.vscode\\mcp.json</code> inside the workspace.</li> </ul> </li> </ol>","tags":["MCP"]},{"location":"2025/09/10/configure-mcp-server-in-visual-studio-and-vs-code/#example-mcpjson-configuration","title":"Example <code>mcp.json</code> configuration","text":"<p>Replace <code>hostname\\sqlexpress</code>, database name, and credentials with your own values.</p> <pre><code>{\n  \"servers\": {\n    \"advworks-db\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@executeautomation/database-server\",\n        \"--sqlserver\",\n        \"--server\", \"hostname\\\\sqlexpress\",\n        \"--database\", \"AdventureWorks\",\n        \"--user\", \"sa\",\n        \"--password\", \"testpwd\"\n      ]\n    }\n  },\n  \"inputs\": []\n}\n</code></pre>","tags":["MCP"]},{"location":"2025/09/10/configure-mcp-server-in-visual-studio-and-vs-code/#visual-studio-configuration","title":"Visual Studio configuration","text":"<ol> <li>Open the GitHub Copilot chat window and choose an agent (for example, GPT-4.1 or Claude).</li> </ol> <ol> <li>Click the tool icon in the chat window and choose \"Select tools (+)\".</li> <li>In the \"Configure MCP Server\" dialog, add the server configuration.</li> </ol> <p>You can configure the MCP server at either the Global level (available to all solutions) or the Solution level (available only to the current solution). The JSON format is the same as the example shown in the VS Code section.</p>","tags":["MCP"]},{"location":"2025/09/10/configure-mcp-server-in-visual-studio-and-vs-code/#example-usage","title":"Example usage","text":"<p>In the GitHub Copilot chat window, select an agent and an LLM model, then ask a query such as:</p> <p>\"List the tables in the Person schema.\"</p> <p>The agent will interact with the MCP database server to run the query and return the list of tables in the <code>Person</code> schema in the chat window.</p>","tags":["MCP"]},{"location":"2025/09/11/using-jupyter-from-vs-code-or-standalone-project/","title":"Using Jupyter from VS code or Standalone project","text":"","tags":["Jupyter Notebook"]},{"location":"2025/09/11/using-jupyter-from-vs-code-or-standalone-project/#vs-code","title":"VS code","text":"<p>To connect a uv-managed project to a Jupyter notebook within VS Code by creating a kernel of the project VS Code requires ipykernel to be present in the project environment.</p> <pre><code># Create a project.\nuv init project\n\n# Move into the project directory.\ncd project\n\n# Add ipykernel as a dev dependency.\nuv add --dev ipykernel\n\n# Open the project in VS Code.\ncode .\n</code></pre> <p>Press  +  + p and select \"Create: New Jupyter Notebook\" It creates .ipynb file and select the python kernel right side of the note book","tags":["Jupyter Notebook"]},{"location":"2025/09/11/using-jupyter-from-vs-code-or-standalone-project/#standalone","title":"Standalone","text":"<p>If you want to standalone project without need pyproject.toml and uv.lock then run the below commands</p> <pre><code>uv venv --seed\nuv pip install pydantic\nuv pip install jupyterlab\n.venv\\Scripts\\jupyter lab\n</code></pre> <p>It launchs the url http://localhost:8888/lab. you can install additional packages via <code>!uv pip install</code>, or even <code>!pip install</code>. To launch the existing notebook again, <code>.venv\\Scripts\\jupyter lab</code></p> <p></p>","tags":["Jupyter Notebook"]},{"location":"2025/09/11/create-a-python-project-using-uv-and-manage-dependencies/","title":"Create a Python Project Using uv and Manage Dependencies","text":"","tags":["uv"]},{"location":"2025/09/11/create-a-python-project-using-uv-and-manage-dependencies/#create-a-python-project","title":"Create a Python Project","text":"<pre><code>uv init pyprojects\n</code></pre> <p>This command automatically creates the following project structure inside the <code>pyprojects</code> folder:</p> <pre><code>.git\n.gitignore\n.python-version\nmain.py\npyproject.toml\nREADME.md\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/create-a-python-project-using-uv-and-manage-dependencies/#pyprojecttoml-content","title":"<code>pyproject.toml</code> Content","text":"<pre><code>[project]\nname = \"pyprojects\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = []\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/create-a-python-project-using-uv-and-manage-dependencies/#run-the-entry-script","title":"Run the Entry Script","text":"<pre><code>uv run main.py\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/create-a-python-project-using-uv-and-manage-dependencies/#install-dependencies","title":"Install Dependencies","text":"<p>Add the latest <code>requests</code> package to <code>pyproject.toml</code>:</p> <pre><code>uv add requests\n</code></pre> <p>This updates the file as follows:</p> <pre><code>dependencies = [\n    \"requests&gt;=2.32.5\",\n]\n</code></pre> <p>Upgrade a package:</p> <pre><code>uv add --upgrade requests\n</code></pre> <p>Remove a package:</p> <pre><code>uv remove requests\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/create-a-python-project-using-uv-and-manage-dependencies/#add-development-dependencies","title":"Add Development Dependencies","text":"<pre><code>uv add --dev pytest\n</code></pre> <p>This creates a <code>[dependency-groups]</code> section in <code>pyproject.toml</code>:</p> <pre><code>[dependency-groups]\ndev = [\n    \"pytest&gt;=8.4.2\",\n]\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/create-a-python-project-using-uv-and-manage-dependencies/#add-dependencies-from-requirementstxt","title":"Add Dependencies from <code>requirements.txt</code>","text":"<pre><code>uv add -r requirements.txt\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/create-a-python-project-using-uv-and-manage-dependencies/#list-packages-in-the-virtual-environment","title":"List Packages in the Virtual Environment","text":"<pre><code>uv pip list\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/create-a-python-project-using-uv-and-manage-dependencies/#lock-and-sync","title":"Lock and Sync","text":"<p>Generate a lock file:</p> <pre><code>uv lock\n</code></pre> <p>Synchronize the environment with the lock file:</p> <pre><code>uv sync\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/","title":"Python Package and Project Manager Tool \u2013 uv","text":"<p>uv is a single tool that replaces <code>pip</code>, <code>pip-tools</code>, <code>pipx</code>, <code>poetry</code>, <code>pyenv</code>, <code>twine</code>, <code>virtualenv</code>, and more.</p> <p>I\u2019ve mostly used <code>pip</code>, <code>pipenv</code>, and <code>virtualenv</code> in the past, but uv combines most of these tasks into one tool. It\u2019s faster, easier to learn, and highly efficient.</p> <p>You can find installation instructions and documentation here: https://docs.astral.sh/uv/</p>","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/#why-uv","title":"Why uv?","text":"<ul> <li>uv is a Python package and project manager that integrates multiple functionalities into one tool.</li> <li>It enables fast dependency installation, virtual environment management, Python version management, and project initialization, all designed to boost productivity.</li> <li>uv can build and publish Python packages to repositories like PyPI, streamlining the process from development to distribution.</li> <li>It automatically creates and manages virtual environments, ensuring clean and isolated project dependencies.</li> </ul>","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/#usage","title":"Usage","text":"","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/#installation","title":"Installation","text":"<p>You can install uv from the command line:</p> <ul> <li>On Windows, use PowerShell.</li> <li>On macOS/Linux, use <code>curl</code>.</li> <li>You can also install it using pipx:</li> </ul> <pre><code>pipx install uv\n</code></pre> <p>Refer to the official documentation for detailed installation options.</p>","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/#verify-installation","title":"Verify installation","text":"<pre><code>uv --version\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/#upgrade-to-the-latest-version","title":"Upgrade to the latest version","text":"<pre><code>uv self update\n# or, if installed via pipx\npipx upgrade uv\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/#install-multiple-python-versions","title":"Install multiple Python versions","text":"<pre><code>uv python install 3.10 3.11 3.12\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/#create-a-virtual-environment-with-a-specific-version","title":"Create a virtual environment with a specific version","text":"<pre><code>uv venv --python 3.12.0\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/#pin-a-specific-python-version-in-the-current-directory","title":"Pin a specific Python version in the current directory","text":"<pre><code>uv python pin 3.11\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/#view-available-and-installed-python-versions","title":"View available and installed Python versions","text":"<pre><code>uv python list\n</code></pre>","tags":["uv"]},{"location":"2025/09/11/python-package-and-project-manager-tool--uv/#remove-the-uv-uvx-and-uvw-binaries","title":"Remove the uv, uvx, and uvw binaries","text":"<p>For Windows, <pre><code>rm $HOME\\.local\\bin\\uv.exe\nrm $HOME\\.local\\bin\\uvx.exe\nrm $HOME\\.local\\bin\\uvw.exe\n</code></pre> For macOS, <pre><code>rm ~/.local/bin/uv ~/.local/bin/uvx\n</code></pre></p>","tags":["uv"]},{"location":"archive/09/2025/","title":"09/2025","text":""},{"location":"category/python/","title":"Python","text":""},{"location":"category/ai/","title":"AI","text":""}]}